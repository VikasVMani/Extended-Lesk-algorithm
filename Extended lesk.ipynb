{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will implement a simple lesk-based WSD. We use SEMCOR WSD dataset for the purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\call2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us define a word class containing the following attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text --> the actual word\n",
    "pos --> POS tag of the word\n",
    "lemma --> Lemma of the word\n",
    "wnsn --> wordnet synset id of the sense used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word:\n",
    "    def __init__(self, text, pos=None, lemma=None, wnsn=None, lexsn=None):\n",
    "        self.text = text\n",
    "        self.pos = pos\n",
    "        self.lemma = lemma\n",
    "        self.wnsn = wnsn\n",
    "        self.lexsn = lexsn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets define Simplified Lesk algorithm first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 84 number of documents \n"
     ]
    }
   ],
   "source": [
    "# let us read a sample xml file\n",
    "tree = ET.parse('semcor/brown2/tagfiles/br-e22.xml')\n",
    "\n",
    "# get the root element\n",
    "root = tree.getroot()\n",
    "\n",
    "documents = []\n",
    "# let us read every sentence, one-by-one\n",
    "# we are ignoring the paragraph structure\n",
    "for sentence_tree in root.findall('context/p/'):\n",
    "    sentence = []\n",
    "#     for every word in that sentence\n",
    "    for word_tree in sentence_tree:\n",
    "#         get the word\n",
    "        word = Word(word_tree.text)\n",
    "        \n",
    "#         if the word xml tag contains info about pos, lemma, wnsn, lexsn, then extract it\n",
    "        if 'pos' in word_tree.attrib:\n",
    "            word.pos = word_tree.attrib['pos']\n",
    "            \n",
    "        if 'lemma' in word_tree.attrib:\n",
    "            word.lemma = word_tree.attrib['lemma']\n",
    "            \n",
    "        if 'wnsn' in word_tree.attrib:\n",
    "            word.wnsn = word_tree.attrib['wnsn']\n",
    "            \n",
    "        if 'lexsn' in word_tree.attrib:\n",
    "            word.lexsn = word_tree.attrib['lexsn']\n",
    "        \n",
    "        sentence.append( word )\n",
    "    documents.append(sentence)\n",
    "\n",
    "print('Read {0} number of documents '.format(len(documents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.5718181818181818\n"
     ]
    }
   ],
   "source": [
    "# To calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# window is chosen as 5, vary the window size\n",
    "window = 5\n",
    "\n",
    "# for every sentence\n",
    "for every_sentence in documents:\n",
    "#     let's get the word in the sentence as a list\n",
    "    sentence = [x.text for x in every_sentence] \n",
    "#     print(\"Sentence is {0}\".format(' '.join(sentence)))\n",
    "\n",
    "# for every word in the sentence\n",
    "    for word_index in range(len(every_sentence)):\n",
    "#         not all words have sense info\n",
    "        if every_sentence[word_index].wnsn is not None:\n",
    "            context_bag = []\n",
    "            every_word = every_sentence[word_index]\n",
    "            \n",
    "            for index in range( max(0, word_index - window), min( word_index + window, len(sentence) ) ):\n",
    "                if index == word_index:\n",
    "                    continue\n",
    "                context_bag.append( sentence[index] )\n",
    "            \n",
    "#             we know the POS tag of the word in the sentence\n",
    "# restrict ourselves to only the senses for that POS category\n",
    "            if every_word.pos.startswith('V'):\n",
    "                synsets = wn.synsets(every_word.text, pos=wn.VERB)\n",
    "            elif every_word.pos.startswith('J'):\n",
    "                synsets = wn.synsets(every_word.text, pos=wn.ADJ)\n",
    "            elif every_word.pos.startswith('R'):\n",
    "                synsets = wn.synsets(every_word.text, pos=wn.ADV)\n",
    "            else:\n",
    "                synsets = wn.synsets(every_word.text, pos=wn.NOUN)\n",
    "                \n",
    "#             all inflections of the word might not be present\n",
    "# search based on lemma\n",
    "            if len(synsets) == 0:\n",
    "                if every_word.pos.startswith('V'):\n",
    "                    synsets = wn.synsets(every_word.lemma, pos=wn.VERB)\n",
    "                elif every_word.pos.startswith('J'):\n",
    "                    synsets = wn.synsets(every_word.lemma, pos=wn.ADJ)\n",
    "                elif every_word.pos.startswith('R'):\n",
    "                    synsets = wn.synsets(every_word.lemma, pos=wn.ADV)\n",
    "                else:\n",
    "                    synsets = wn.synsets(every_word.lemma, pos=wn.NOUN)\n",
    "                \n",
    "            if len(synsets) == 0:\n",
    "                continue\n",
    "            \n",
    "#             find the best synset based on simple word-overlap between word context and synset examples\n",
    "            synset_score = -100\n",
    "            synset_id = \"\"\n",
    "            for every_synset in synsets:\n",
    "                synset_bag = []\n",
    "                for every_synset_example in every_synset.examples():\n",
    "                    synset_bag.extend( every_synset_example.split(' ') )\n",
    "                \n",
    "                matching_words = list( set( context_bag ).intersection( set(synset_bag) ) )\n",
    "                if len(matching_words) > synset_score:\n",
    "                    synset_score = len(matching_words)\n",
    "                    synset_id = every_synset.name().split('.')[-1]\n",
    "                    \n",
    "            if synset_id.startswith('0'):\n",
    "                synset_id = synset_id[1:]\n",
    "\n",
    "#             print('Best matching synset id is {0} with overlapping words {1}'.format( synset_id, synset_score ))\n",
    "#             print('Actual synset id is {0}'.format( every_word.wnsn ))\n",
    "            \n",
    "            if synset_id == every_word.wnsn:\n",
    "                correct = correct + 1\n",
    "            total = total + 1\n",
    "\n",
    "print('Accuracy is {0}'.format( (correct * 1.0)/ (total * 1.0) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will define Extended Lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def lesk(context_sentence, ambiguous_word, pos=None, hyperhypo=False):\n",
    "    max_overlaps = 0; lesk_sense = None\n",
    "    context_sentence = context_sentence.split()\n",
    "    for ss in wn.synsets(ambiguous_word):\n",
    "        # If POS is specified.\n",
    "        if pos and ss.pos is not pos:\n",
    "            continue\n",
    "\n",
    "        lesk_dictionary = []\n",
    "\n",
    "        # Includes definition.\n",
    "        lesk_dictionary+= ss.definition().split()\n",
    "        # Includes lemma_names.\n",
    "        lesk_dictionary+= ss.lemma_names()\n",
    "\n",
    "        # Optional: includes lemma_names of hypernyms and hyponyms.\n",
    "        if hyperhypo == True:\n",
    "            lesk_dictionary+= list(chain(*[i.lemma_names() for i in ss.hypernyms()+ss.hyponyms()]))       \n",
    "\n",
    "\n",
    "        overlaps = set(lesk_dictionary).intersection(context_sentence)\n",
    "\n",
    "        if len(overlaps) > max_overlaps:\n",
    "            lesk_sense = ss\n",
    "            max_overlaps = len(overlaps)\n",
    "    return lesk_sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of stopwords: 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "print(\"number of stopwords:\", len(stop_words))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(filename):\n",
    "    tree = ET.parse('semcor/brown2/tagfiles/'+filename)\n",
    "\n",
    "    # get the root element\n",
    "    root = tree.getroot()\n",
    "\n",
    "    documents = []\n",
    "    # let us read every sentence, one-by-one\n",
    "    # we are ignoring the paragraph structure\n",
    "    for sentence_tree in root.findall('context/p/'):\n",
    "        sentence = []\n",
    "    #     for every word in that sentence\n",
    "        for word_tree in sentence_tree:\n",
    "    #         get the word\n",
    "            word = Word(word_tree.text)\n",
    "\n",
    "    #         if the word xml tag contains info about pos, lemma, wnsn, lexsn, then extract it\n",
    "            if 'pos' in word_tree.attrib:\n",
    "                word.pos = word_tree.attrib['pos']\n",
    "\n",
    "            if 'lemma' in word_tree.attrib:\n",
    "                word.lemma = word_tree.attrib['lemma']\n",
    "\n",
    "            if 'wnsn' in word_tree.attrib:\n",
    "                word.wnsn = word_tree.attrib['wnsn']\n",
    "\n",
    "            if 'lexsn' in word_tree.attrib:\n",
    "                word.lexsn = word_tree.attrib['lexsn']\n",
    "\n",
    "            sentence.append( word )\n",
    "        documents.append(sentence)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # window is chosen as 5, vary the window size\n",
    "    window = 5\n",
    "\n",
    "    # for every sentence\n",
    "    for every_sentence in documents:\n",
    "    #     let's get the word in the sentence as a list\n",
    "        sentence = [x.text for x in every_sentence] \n",
    "    #     print(\"Sentence is {0}\".format(' '.join(sentence)))\n",
    "\n",
    "    # for every word in the sentence\n",
    "        for word_index in range(len(every_sentence)):\n",
    "    #         not all words have sense info\n",
    "            if every_sentence[word_index].wnsn is not None and every_sentence[word_index].pos.startswith('N')==False:\n",
    "                context_bag = ''\n",
    "                every_word = every_sentence[word_index]\n",
    "\n",
    "                for index in range( max(0, word_index - window), min( word_index + window, len(sentence) ) ):\n",
    "                    if index == word_index:\n",
    "                        continue\n",
    "                    context_bag += sentence[index] +' '\n",
    "                synsetname = lesk(context_bag, every_word.text)\n",
    "                if synsetname is None:\n",
    "                    continue\n",
    "                synset_id = synsetname.name().split('.')[-1]\n",
    "                if synset_id.startswith('0'):\n",
    "                            synset_id = synset_id[1:]\n",
    "                if synset_id == every_word.wnsn:\n",
    "                    correct = correct + 1\n",
    "                total = total + 1\n",
    "    print('Accuracy is {0}'.format( (correct * 1.0)/ (total * 1.0) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we will check for every file in brown2 folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the File Name : br-e22.xml\n",
      "Accuracy is 0.42643391521197005\n",
      "For the File Name : br-e23.xml\n",
      "Accuracy is 0.4609571788413098\n",
      "For the File Name : br-e25.xml\n",
      "Accuracy is 0.4398976982097187\n",
      "For the File Name : br-e26.xml\n",
      "Accuracy is 0.40934065934065933\n",
      "For the File Name : br-e27.xml\n",
      "Accuracy is 0.5430711610486891\n",
      "For the File Name : br-e28.xml\n",
      "Accuracy is 0.40641711229946526\n",
      "For the File Name : br-e30.xml\n",
      "Accuracy is 0.4470588235294118\n",
      "For the File Name : br-e31.xml\n",
      "Accuracy is 0.42441860465116277\n",
      "For the File Name : br-f08.xml\n",
      "Accuracy is 0.450402144772118\n",
      "For the File Name : br-f13.xml\n",
      "Accuracy is 0.4461942257217848\n",
      "For the File Name : br-f14.xml\n",
      "Accuracy is 0.3961218836565097\n",
      "For the File Name : br-f15.xml\n",
      "Accuracy is 0.4077922077922078\n",
      "For the File Name : br-f16.xml\n",
      "Accuracy is 0.410958904109589\n",
      "For the File Name : br-f17.xml\n",
      "Accuracy is 0.42574257425742573\n",
      "For the File Name : br-f18.xml\n",
      "Accuracy is 0.47096774193548385\n",
      "For the File Name : br-f20.xml\n",
      "Accuracy is 0.4222222222222222\n",
      "For the File Name : br-f21.xml\n",
      "Accuracy is 0.43597560975609756\n",
      "For the File Name : br-f22.xml\n",
      "Accuracy is 0.396011396011396\n",
      "For the File Name : br-f23.xml\n",
      "Accuracy is 0.5145631067961165\n",
      "For the File Name : br-f24.xml\n",
      "Accuracy is 0.4148606811145511\n",
      "For the File Name : br-f25.xml\n",
      "Accuracy is 0.559610705596107\n",
      "For the File Name : br-f33.xml\n",
      "Accuracy is 0.4573643410852713\n",
      "For the File Name : br-f44.xml\n",
      "Accuracy is 0.47044334975369456\n",
      "For the File Name : br-g12.xml\n",
      "Accuracy is 0.40331491712707185\n",
      "For the File Name : br-g14.xml\n",
      "Accuracy is 0.4\n",
      "For the File Name : br-g16.xml\n",
      "Accuracy is 0.41626794258373206\n",
      "For the File Name : br-g17.xml\n",
      "Accuracy is 0.4672131147540984\n",
      "For the File Name : br-g18.xml\n",
      "Accuracy is 0.41643835616438357\n",
      "For the File Name : br-g19.xml\n",
      "Accuracy is 0.4607329842931937\n",
      "For the File Name : br-g20.xml\n",
      "Accuracy is 0.44611528822055135\n",
      "For the File Name : br-g21.xml\n",
      "Accuracy is 0.39941690962099125\n",
      "For the File Name : br-g22.xml\n",
      "Accuracy is 0.39901477832512317\n",
      "For the File Name : br-g23.xml\n",
      "Accuracy is 0.5142857142857142\n",
      "For the File Name : br-g28.xml\n",
      "Accuracy is 0.4375\n",
      "For the File Name : br-g31.xml\n",
      "Accuracy is 0.4218289085545723\n",
      "For the File Name : br-g39.xml\n",
      "Accuracy is 0.3387096774193548\n",
      "For the File Name : br-g43.xml\n",
      "Accuracy is 0.4573643410852713\n",
      "For the File Name : br-g44.xml\n",
      "Accuracy is 0.46835443037974683\n",
      "For the File Name : br-h09.xml\n",
      "Accuracy is 0.4722222222222222\n",
      "For the File Name : br-h11.xml\n",
      "Accuracy is 0.4744744744744745\n",
      "For the File Name : br-h12.xml\n",
      "Accuracy is 0.4940828402366864\n",
      "For the File Name : br-h13.xml\n",
      "Accuracy is 0.4037433155080214\n",
      "For the File Name : br-h14.xml\n",
      "Accuracy is 0.47214854111405835\n",
      "For the File Name : br-h15.xml\n",
      "Accuracy is 0.4268292682926829\n",
      "For the File Name : br-h16.xml\n",
      "Accuracy is 0.45084745762711864\n",
      "For the File Name : br-h17.xml\n",
      "Accuracy is 0.42105263157894735\n",
      "For the File Name : br-h18.xml\n",
      "Accuracy is 0.40463917525773196\n",
      "For the File Name : br-h21.xml\n",
      "Accuracy is 0.4082687338501292\n",
      "For the File Name : br-h24.xml\n",
      "Accuracy is 0.4174174174174174\n",
      "For the File Name : br-j29.xml\n",
      "Accuracy is 0.41624365482233505\n",
      "For the File Name : br-j30.xml\n",
      "Accuracy is 0.38578680203045684\n",
      "For the File Name : br-j31.xml\n",
      "Accuracy is 0.47174447174447176\n",
      "For the File Name : br-j32.xml\n",
      "Accuracy is 0.4186046511627907\n",
      "For the File Name : br-j33.xml\n",
      "Accuracy is 0.3961038961038961\n",
      "For the File Name : br-j34.xml\n",
      "Accuracy is 0.5141509433962265\n",
      "For the File Name : br-j35.xml\n",
      "Accuracy is 0.44155844155844154\n",
      "For the File Name : br-j38.xml\n",
      "Accuracy is 0.5125284738041003\n",
      "For the File Name : br-j41.xml\n",
      "Accuracy is 0.47642679900744417\n",
      "For the File Name : br-j42.xml\n",
      "Accuracy is 0.4519230769230769\n",
      "For the File Name : br-l08.xml\n",
      "Accuracy is 0.378698224852071\n",
      "For the File Name : br-l09.xml\n",
      "Accuracy is 0.3770491803278688\n",
      "For the File Name : br-l10.xml\n",
      "Accuracy is 0.4181184668989547\n",
      "For the File Name : br-l13.xml\n",
      "Accuracy is 0.4\n",
      "For the File Name : br-l14.xml\n",
      "Accuracy is 0.4307228915662651\n",
      "For the File Name : br-l15.xml\n",
      "Accuracy is 0.425531914893617\n",
      "For the File Name : br-l16.xml\n",
      "Accuracy is 0.41359773371104813\n",
      "For the File Name : br-l17.xml\n",
      "Accuracy is 0.41214057507987223\n",
      "For the File Name : br-l18.xml\n",
      "Accuracy is 0.4161490683229814\n",
      "For the File Name : br-n09.xml\n",
      "Accuracy is 0.37267080745341613\n",
      "For the File Name : br-n10.xml\n",
      "Accuracy is 0.43322475570032576\n",
      "For the File Name : br-n11.xml\n",
      "Accuracy is 0.39457831325301207\n",
      "For the File Name : br-n12.xml\n",
      "Accuracy is 0.39805825242718446\n",
      "For the File Name : br-n14.xml\n",
      "Accuracy is 0.3877005347593583\n",
      "For the File Name : br-n15.xml\n",
      "Accuracy is 0.3757396449704142\n",
      "For the File Name : br-n16.xml\n",
      "Accuracy is 0.3710144927536232\n",
      "For the File Name : br-n17.xml\n",
      "Accuracy is 0.4200626959247649\n",
      "For the File Name : br-n20.xml\n",
      "Accuracy is 0.4147727272727273\n",
      "For the File Name : br-p07.xml\n",
      "Accuracy is 0.43352601156069365\n",
      "For the File Name : br-p09.xml\n",
      "Accuracy is 0.3862433862433862\n",
      "For the File Name : br-p10.xml\n",
      "Accuracy is 0.4154727793696275\n",
      "For the File Name : br-p12.xml\n",
      "Accuracy is 0.42071197411003236\n",
      "For the File Name : br-p24.xml\n",
      "Accuracy is 0.4646840148698885\n",
      "For the File Name : br-r04.xml\n",
      "Accuracy is 0.3630952380952381\n"
     ]
    }
   ],
   "source": [
    "from os import walk\n",
    "\n",
    "filenames = next(walk(\"semcor/brown2/tagfiles/\"), (None, None, []))[2] \n",
    "\n",
    "for fileName in filenames:\n",
    "    print(\"For the File Name :\" , fileName)\n",
    "    accuracy(fileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The task for you is:\n",
    "\n",
    "#### 1. Use both gloss and example sentences for producing the context signature from synsets against which you aim to match the sentence context.\n",
    "#### 2. Extend this approach over all the files present in the sense-tagged Brown corpus provided in the brown2 folder.\n",
    "#### 3. Ensure stop word removal\n",
    "#### 4. Try to improve this approach using the Extended Lesk algorithm. The algorithm was discussed in class.\n",
    "#### HINT: Include hypernymy & hyponymy synsets' gloss and example sentences in the signature overlap."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
